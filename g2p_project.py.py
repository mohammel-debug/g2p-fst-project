# -*- coding: utf-8 -*-
"""Final_project_morph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CC-w-xX-avTlypclSqrjP0eziChnDbVP

# **title**

Authors:

Colab Notebook:

GitHub Link:

Presentation:

# dataset
"""

#Installation of Pyfoma
!pip install pyfoma

#from google.colab import drive
#drive.mount('/content/drive')

!wget https://raw.githubusercontent.com/open-dict-data/ipa-dict/master/data/en_UK.txt

import pandas as pd

ipa_dict_en_uk = pd.read_csv("en_UK.txt", sep="\t", header=None, names=["word", "ipa"])

display(ipa_dict_en_uk.shape)
ipa_dict_en_uk.head()

lexicon = []
with open("en_UK.txt", encoding="utf-8") as f:
    for line in f:
        parts = line.strip().split("\t")
        if len(parts) == 2:
            word, ipa = parts
            ipa = ipa.strip("/")
            lexicon.append((word, ipa))

print(lexicon[:5])

#Test Data of UK english IPA phonetic transcriptions

from pyfoma import FST
#fsts = {}
#fsts['t']=FST.from_strings(open("/content/drive/MyDrive/en_UK.txt"))
# esto creo no va a funcionar porque está separado con "tab". Osea trata the whole string como si fuera una linea
#fsts['t']=FST.from_strings(open("ipa_dict_en_uk"))


fsts = {}
original_lexicon = FST.from_strings(lexicon) # this will be used just to create the original and correct ipa version
lexicon_for_composition = FST.from_strings(lexicon)
#while this, we will use it for compisition, which was the main problem.
#Because when we use @ everything become mixed thats why we wwere gwtting the same output.

list(original_lexicon.generate('car'))

print(list(original_lexicon.generate('yellow')))

"""im wondering about the use of this funciton. im a bit consufed. why do we need it?"""

#function to transform IPA into alphabet

def ipa_to_text(ipa_transcription):
  rules = {
      'eɪ': 'ay', 'aɪ': 'uy', 'ɔɪ': 'oy', 'aʊ': 'ow', 'oʊ': 'o',
        'tʃ': 'ch', 'dʒ': 'j', 'ʃ': 'sh', 'ʒ': 's', 'θ': 'th', 'ð': 'th',
        'ŋ': 'ng',


        'æ': 'a', 'ɑː': 'a', 'ɛ': 'e', 'ɪ': 'i', 'iː': 'ee',
        'ɒ': 'o', 'ɔː': 'o', 'ʊ': 'u', 'uː': 'oo', 'ʌ': 'u', 'ə': 'e',


        'p': 'p', 'b': 'b', 't': 't', 'd': 'd', 'k': 'k', 'g': 'g',
        'f': 'f', 'v': 'v', 's': 's', 'z': 'z', 'm': 'm', 'n': 'n',
        'h': 'h', 'l': 'l', 'r': 'r', 'w': 'w', 'j': 'y'
    }

  result=ipa_transcription.lower()

  for char in "ˈˌ'":
    result=result.replace(char, "")

  for phoneme, grapheme in rules.items():
    result=result.replace(phoneme, grapheme)

  return result

print(ipa_to_text("dɪə"))

#Rules

#IMPORTANT: we have to deal with long vowels here as well. It seems like when we write a:, it is considered as 2 characters not only one.

fsts['rule_pre'] = FST.re("$^rewrite('ˈ':'' | 'ˌ':'')")

#consonants
fsts['rule_ð'] = FST.re("$^rewrite(ð:d)")  #that, dat / father, fader
fsts['rule_θ'] = FST.re("$^rewrite(θ:t)")  # thought, tot
fsts['rule_v'] = FST.re("$^rewrite(v:b)")  #verb, berb
fsts['rule_w'] = FST.re("$^rewrite(w:(gu))")  #win, guin
#fsts['rule_h_drop'] = FST.re("$^rewrite((h): )")  #helen, Elen?
#we cannot put 2 rules for one letter ;( - this comment seems to indicate a constraint or a known issue. With the next h rule I think it is enough, I would say that we say jelen rater than elen
fsts['rule_h'] = FST.re("$^rewrite(h:x)")  #heavy, jeby/hello,jelo
fsts['rule_ŋ'] = FST.re("$^rewrite(ŋ:n)")  #sing, sin
fsts['rule_r'] = FST.re("$^rewrite(ɹ:r)")  # in cases like part, we don't say pa:t but paRt. I don't know if this is the way to put it though
fsts['rule_j'] = FST.re("$^rewrite(j:ʝ)")  #you, ju / yes,jes / yellow,jelow
fsts['rule_z'] = FST.re("$^rewrite(z:s)")  #lazy, leisi
fsts['rule_ʒ'] = FST.re("$^rewrite(ʒ:ʃ)")  #leasure, lisher
fsts['rule_ʊ'] = FST.re("$^rewrite(ʊ:u)") # full not fʊl but ful

#diphthongs
fsts['rule_eɪ'] = FST.re("$^rewrite(eɪ:ei)")  # day, de
fsts['rule_oʊ'] = FST.re("$^rewrite(oʊ:ou)")  # home, houm
fsts['rule_əʊ'] = FST.re("$^rewrite(əʊ:ou)")  # go, go
fsts['rule_aɪ'] = FST.re("$^rewrite(aɪ:ai)") # time, taim
fsts['rule_aʊ'] = FST.re("$^rewrite(aʊ:au)")  # about, abaut
fsts['rule_ɔɪ'] = FST.re("$^rewrite(ɔɪ:oi)") # noise, nois
fsts['rule_ʊə'] = FST.re("$^rewrite(ʊə:ua)") # tour, tuar
fsts['rule_eə'] = FST.re("$^rewrite(eə:ea)") # hair, jear


#vowels
fsts['rule_iː'] = FST.re("$^rewrite((iː):i)")  # see, si
fsts['rule_ɪ'] = FST.re("$^rewrite(ɪ:i)")  # shIt, shit
fsts['rule_æ'] = FST.re("$^rewrite(æ:a)")  # cat, cat
fsts['rule_ʌ'] = FST.re("$^rewrite(ʌ:o)")  # cup, cop I believe this is more /a/ than /o/
fsts['rule_ɜː'] = FST.re("$^rewrite((ɜː):e)")  # bird, berd not working
fsts['rule_ə'] = FST.re("$^rewrite(ə:a / _ #)")  # about, abaut. not working
fsts['rule_ə1'] = FST.re("$^rewrite(ə:o / # _ n)")
fsts['rule_uː'] = FST.re("$^rewrite((uː):u)")  # you, ju
fsts['rule_ɔː'] = FST.re("$^rewrite((ɔː):o)")  # thought, tot
fsts['rule_ɑː'] = FST.re("$^rewrite((ɑː):a)")  # car, car / bark, bark ...this is problematic as well. The rs are pronounced in English_Spanish

#from now on, we can add more sophisticated rules

# Define a set for consonants
fsts['C'] = FST.re("b|c|d|f|g|h|j|k|l|m|n|p|q|r|s|t|v|w|x|y|z")

# Rewrite rule for 's' at the beginning of a word, followed by a consonant
fsts['rule_s'] = FST.re("$^rewrite(s:(es) / # _ $C)", fsts) # The '$C' refers to the FST named 'C' in the fsts dictionary

# full, ful / final, final(dark l does not exist in the end of words in spanish. It's pronounced as normal l)
fsts['rule_ɫ'] = FST.re("$^rewrite(ɫ:l)")

#Spanish does not have voiced stops/fricatives at the end of syllables
fsts['rule_b_end'] = FST.re("$^rewrite(b:p / _ #)")  # club, clup
fsts['rule_g_end'] = FST.re("$^rewrite(ɡ:k / _ #)")  # bag, bak not working...now yes- the problem was this: g instead of ɡ.
fsts['rule_v_end'] = FST.re("$^rewrite(v:f / _ #)")  # have, haf not working

'''#Grammar to transform English pronunciation into Spanish

from pyfoma import *
fsts = {}
fsts['V'] = FST.re("a|e|i|o|u")
#fsts['vowelrule'] = FST.re("a|e|i|o|u : a|e|i|o|u")
fsts['irule'] = FST.re("$^rewrite((iː): i)")
fsts['aerule'] = FST.re("$^rewrite((æ): a)")
fsts['uhrule'] = FST.re("$^rewrite((ʌ): o|a)") #depends on the spanish variety, argentina chile spain /a/ venezuela mexico and central america /o/
fsts['longvrule'] = FST.re("$^rewrite((ɜː): e)")
fsts['schwarule'] = FST.re("$^rewrite(ɐ: $V)", fsts) #don't know how to do it so that in "about" the only output is about, not ebout, ibout, obout and ubout
#Lo puedes ponerlo directametne y ya esta. Lo he puesto arriba. --> fsts['schwarule'] = FST.re("$^rewrite(ɐ: a)
fsts['dentfricrule1'] = FST.re("$^rewrite(ð:d)")
fsts['dentfricrule2'] = FST.re("$^rewrite()")

#need to do rrule, every r would be pronounced by spanish speakers: bird is berd, work is werk, player is pleieR
fsts['grammar'] = FST.re(" $irule @ $aerule @ $uhrule @ $longvrule @ $schwarule", fsts)'''

grammar = FST.re(
    "$rule_pre @ $rule_ð @ $rule_θ @ $rule_v @ $rule_w @ $rule_h @ $rule_r @ "
    "$rule_ŋ @ $rule_j @ $rule_z @ $rule_ʒ @ $rule_eɪ @ $rule_oʊ @ $rule_əʊ @ "
    "$rule_aɪ @ $rule_aʊ @ $rule_ɔɪ @ "
    "$rule_ʊə @ $rule_eə @ "
    "$rule_iː @ $rule_ɪ @ $rule_æ @ $rule_ʌ @ $rule_ɜː @ $rule_ə @ $rule_ə1 @ $rule_ʊ @ "
    "$rule_uː @ $rule_ɔː @ $rule_ɑː @ $rule_s @ $rule_ɫ @ $rule_b_end @ $rule_g_end @ $rule_v_end", fsts)

#With this, it seems to work!!
all_together = lexicon_for_composition @ grammar

#small test
print(list(original_lexicon.generate('car')))
print(list(all_together.apply("car")))

print(list(original_lexicon.generate('destroy')))
print(list(all_together.generate("destroy")))

print(list(original_lexicon.generate('school')))
print(list(all_together.apply("school")))

print(list(original_lexicon.generate('very')))
print(list(all_together.apply("very")))

print(list(original_lexicon.generate('go')))
print(list(all_together.apply("go")))
#here, we have a problem as well. the rule is (oʊ:o),
#but it seems like to be a conflict between diphthongs and vowels

print(list(original_lexicon.generate('about')))
print(list(all_together.apply("about")))

print(list(original_lexicon.generate('go')))
print(list(all_together.generate('go')))

test_sample = ipa_dict_en_uk.sample(n=500, random_state=42) #using the same dataset. We can use a different one later if u wish

results = []

print(f"{'WORD':<20} | {'IPA':<20} | {'SPANISH-ENGLISH'}")
print("-" * 60)

for word in test_sample['word']:
    try:
        ipa_list = list(original_lexicon.generate(word))
        ipa = ipa_list[0] if ipa_list else "N/A"

        transformed_list = list(all_together.generate(word))
        transformed = transformed_list[0] if transformed_list else "N/A"

        results.append({'word': word, 'ipa': ipa, 'transformed': transformed})

        if len(results) <= 30:
            print(f"{word:<20} | {ipa:<20} | {transformed}")

    except Exception as e:
        continue

test_results_df = pd.DataFrame(results)
test_results_df.head()